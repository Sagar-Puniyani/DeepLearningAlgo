{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[8,8,4],[7,9,5],[6,10,6],[5,12,7]], columns=['cgpa', 'profile_score', 'lpa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)         \n",
    "\n",
    "    for l in range(1, L):\n",
    "\n",
    "        parameters['W' + str(l)] = np.ones((layer_dims[l-1], layer_dims[l]))*0.1\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[0.1, 0.1],\n",
       "        [0.1, 0.1]]),\n",
       " 'b1': array([[0.],\n",
       "        [0.]]),\n",
       " 'W2': array([[0.1],\n",
       "        [0.1]]),\n",
       " 'b2': array([[0.]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialize_parameters([2,2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A_prev, W, b):\n",
    "    \n",
    "    Z = np.dot(W.T, A_prev) + b\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Prop\n",
    "def L_layer_forward(X, parameters):\n",
    "\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    for l in range(1, L+1):\n",
    "        A_prev = A \n",
    "        Wl = parameters['W' + str(l)]\n",
    "        bl = parameters['b' + str(l)]\n",
    "        # print(\"A\"+str(l-1)+\": \", A_prev)\n",
    "        # print(\"W\"+str(l)+\": \", Wl)\n",
    "        # print(\"b\"+str(l)+\": \", bl)\n",
    "        # print(\"--\"*20)\n",
    "\n",
    "    A = linear_forward(A_prev, Wl, bl)\n",
    "    # print(\"A\"+str(l)+\": \", A)\n",
    "    # print(\"**\"*20)\n",
    "            \n",
    "    return A,A_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A0:  [[8]\n",
      " [8]]\n",
      "W1:  [[0.1 0.1]\n",
      " [0.1 0.1]]\n",
      "b1:  [[0.]\n",
      " [0.]]\n",
      "----------------------------------------\n",
      "A1:  [[8]\n",
      " [8]]\n",
      "W2:  [[0.1]\n",
      " [0.1]]\n",
      "b2:  [[0.]]\n",
      "----------------------------------------\n",
      "A2:  [[1.6]]\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "X = df[['cgpa', 'profile_score']].values[0].reshape(2,1) # Shape(no of features, no. of training example)\n",
    "y = df[['lpa']].values[0][0]\n",
    "\n",
    "# Parameter initialization\n",
    "parameters = initialize_parameters([2,2,1])\n",
    "\n",
    "y_hat,A1 = L_layer_forward(X, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.5424"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y-0.32)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.70061951634344"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters,y,y_hat,A1,X):\n",
    "  parameters['W2'][0][0] = parameters['W2'][0][0] + (0.001 * 2 * (y - y_hat)*A1[0][0])\n",
    "  parameters['W2'][1][0] = parameters['W2'][1][0] + (0.001 * 2 * (y - y_hat)*A1[1][0])\n",
    "  parameters['b2'][0][0] = parameters['W2'][1][0] + (0.001 * 2 * (y - y_hat))\n",
    "\n",
    "  parameters['W1'][0][0] = parameters['W1'][0][0] + (0.001 * 2 * (y - y_hat)*parameters['W2'][0][0]*X[0][0])\n",
    "  parameters['W1'][0][1] = parameters['W1'][0][1] + (0.001 * 2 * (y - y_hat)*parameters['W2'][0][0]*X[1][0])\n",
    "  parameters['b1'][0][0] = parameters['b1'][0][0] + (0.001 * 2 * (y - y_hat)*parameters['W2'][0][0])\n",
    "\n",
    "  parameters['W1'][1][0] = parameters['W1'][1][0] + (0.001 * 2 * (y - y_hat)*parameters['W2'][1][0]*X[0][0])\n",
    "  parameters['W1'][1][1] = parameters['W1'][1][1] + (0.001 * 2 * (y - y_hat)*parameters['W2'][1][0]*X[1][0])\n",
    "  parameters['b1'][1][0] = parameters['b1'][1][0] + (0.001 * 2 * (y - y_hat)*parameters['W2'][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A0:  [[8]\n",
      " [8]]\n",
      "W1:  [[0.1 0.1]\n",
      " [0.1 0.1]]\n",
      "b1:  [[0.]\n",
      " [0.]]\n",
      "----------------------------------------\n",
      "A1:  [[8]\n",
      " [8]]\n",
      "W2:  [[0.1]\n",
      " [0.1]]\n",
      "b2:  [[0.]]\n",
      "----------------------------------------\n",
      "A2:  [[1.6]]\n",
      "****************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'W1': array([[0.10531456, 0.10531456],\n",
       "        [0.10531456, 0.10531456]]),\n",
       " 'b1': array([[0.00066432],\n",
       "        [0.00066432]]),\n",
       " 'W2': array([[0.1384],\n",
       "        [0.1384]]),\n",
       " 'b2': array([[0.1432]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[['cgpa', 'profile_score']].values[0].reshape(2,1) # Shape(no of features, no. of training example)\n",
    "y = df[['lpa']].values[0][0]\n",
    "\n",
    "# Parameter initialization\n",
    "parameters = initialize_parameters([2,2,1])\n",
    "\n",
    "y_hat,A1 = L_layer_forward(X,parameters)\n",
    "y_hat = y_hat[0][0]\n",
    "\n",
    "update_parameters(parameters,y,y_hat,A1,X)\n",
    "\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A0:  [[7]\n",
      " [9]]\n",
      "W1:  [[0.10531456 0.10531456]\n",
      " [0.10531456 0.10531456]]\n",
      "b1:  [[0.00066432]\n",
      " [0.00066432]]\n",
      "----------------------------------------\n",
      "A1:  [[7]\n",
      " [9]]\n",
      "W2:  [[0.1384]\n",
      " [0.1384]]\n",
      "b2:  [[0.1432]]\n",
      "----------------------------------------\n",
      "A2:  [[2.3576]]\n",
      "****************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'W1': array([[0.111803  , 0.11365684],\n",
       "        [0.11219401, 0.11415956]]),\n",
       " 'b1': array([[0.00159124],\n",
       "        [0.0016471 ]]),\n",
       " 'W2': array([[0.1753936],\n",
       "        [0.1859632]]),\n",
       " 'b2': array([[0.191248]])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[['cgpa', 'profile_score']].values[1].reshape(2,1) # Shape(no of features, no. of training exaplme)\n",
    "y = df[['lpa']].values[1][0]\n",
    "\n",
    "y_hat,A1 = L_layer_forward(X,parameters)\n",
    "y_hat = y_hat[0][0]\n",
    "\n",
    "update_parameters(parameters,y,y_hat,A1,X)\n",
    "\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch -  1 Loss -  7.205413203742441\n",
      "Epoch -  2 Loss -  1.0733686445728834\n",
      "Epoch -  3 Loss -  0.9583801635331537\n",
      "Epoch -  4 Loss -  0.9721506091746084\n",
      "Epoch -  5 Loss -  0.9426118575983916\n",
      "Epoch -  6 Loss -  0.9001561889946194\n",
      "Epoch -  7 Loss -  0.8562395613931871\n",
      "Epoch -  8 Loss -  0.813754659635432\n",
      "Epoch -  9 Loss -  0.7733115003785209\n",
      "Epoch -  10 Loss -  0.7349655914480541\n",
      "Epoch -  11 Loss -  0.6986440529883167\n",
      "Epoch -  12 Loss -  0.6642479287554847\n",
      "Epoch -  13 Loss -  0.6316764162404747\n",
      "Epoch -  14 Loss -  0.6008324193552801\n",
      "Epoch -  15 Loss -  0.5716236511675292\n",
      "Epoch -  16 Loss -  0.5439626859081297\n",
      "Epoch -  17 Loss -  0.5177667714541684\n",
      "Epoch -  18 Loss -  0.49295759539182915\n",
      "Epoch -  19 Loss -  0.4694610501917268\n",
      "Epoch -  20 Loss -  0.44720700781999134\n",
      "Epoch -  21 Loss -  0.42612910572999907\n",
      "Epoch -  22 Loss -  0.4061645442068723\n",
      "Epoch -  23 Loss -  0.3872538945923119\n",
      "Epoch -  24 Loss -  0.3693409178362933\n",
      "Epoch -  25 Loss -  0.35237239282656535\n",
      "Epoch -  26 Loss -  0.336297953970446\n",
      "Epoch -  27 Loss -  0.3210699375303265\n",
      "Epoch -  28 Loss -  0.3066432362408327\n",
      "Epoch -  29 Loss -  0.2929751617609866\n",
      "Epoch -  30 Loss -  0.2800253145388096\n",
      "Epoch -  31 Loss -  0.2677554606885649\n",
      "Epoch -  32 Loss -  0.2561294155024524\n",
      "Epoch -  33 Loss -  0.24511293323893876\n",
      "Epoch -  34 Loss -  0.23467360284923153\n",
      "Epoch -  35 Loss -  0.22478074932166134\n",
      "Epoch -  36 Loss -  0.21540534034101655\n",
      "Epoch -  37 Loss -  0.2065198979762102\n",
      "Epoch -  38 Loss -  0.1980984151251481\n",
      "Epoch -  39 Loss -  0.1901162764602606\n",
      "Epoch -  40 Loss -  0.18255018363203146\n",
      "Epoch -  41 Loss -  0.175378084500921\n",
      "Epoch -  42 Loss -  0.16857910618050517\n",
      "Epoch -  43 Loss -  0.16213349168631733\n",
      "Epoch -  44 Loss -  0.15602253999601773\n",
      "Epoch -  45 Loss -  0.15022854933696536\n",
      "Epoch -  46 Loss -  0.1447347635272071\n",
      "Epoch -  47 Loss -  0.1395253212052756\n",
      "Epoch -  48 Loss -  0.1345852077930738\n",
      "Epoch -  49 Loss -  0.12990021004451718\n",
      "Epoch -  50 Loss -  0.1254568730405569\n",
      "Epoch -  51 Loss -  0.12124245949872488\n",
      "Epoch -  52 Loss -  0.1172449112724489\n",
      "Epoch -  53 Loss -  0.11345281292211938\n",
      "Epoch -  54 Loss -  0.10985535724625746\n",
      "Epoch -  55 Loss -  0.10644231266714871\n",
      "Epoch -  56 Loss -  0.10320399237101084\n",
      "Epoch -  57 Loss -  0.1001312251081432\n",
      "Epoch -  58 Loss -  0.09721532756362611\n",
      "Epoch -  59 Loss -  0.09444807821393514\n",
      "Epoch -  60 Loss -  0.09182169258941648\n",
      "Epoch -  61 Loss -  0.08932879986688365\n",
      "Epoch -  62 Loss -  0.08696242072067903\n",
      "Epoch -  63 Loss -  0.08471594636440534\n",
      "Epoch -  64 Loss -  0.08258311871918972\n",
      "Epoch -  65 Loss -  0.08055801164780788\n",
      "Epoch -  66 Loss -  0.07863501319725082\n",
      "Epoch -  67 Loss -  0.0768088087954352\n",
      "Epoch -  68 Loss -  0.07507436535066407\n",
      "Epoch -  69 Loss -  0.07342691620522993\n",
      "Epoch -  70 Loss -  0.07186194689716652\n",
      "Epoch -  71 Loss -  0.07037518168663806\n",
      "Epoch -  72 Loss -  0.06896257080579789\n",
      "Epoch -  73 Loss -  0.06762027839317167\n",
      "Epoch -  74 Loss -  0.06634467107570954\n",
      "Epoch -  75 Loss -  0.06513230716365705\n",
      "Epoch -  76 Loss -  0.06397992642525212\n",
      "Epoch -  77 Loss -  0.062884440410048\n",
      "Epoch -  78 Loss -  0.06184292329133529\n",
      "Epoch -  79 Loss -  0.06085260319973818\n",
      "Epoch -  80 Loss -  0.059910854021549026\n",
      "Epoch -  81 Loss -  0.05901518763680476\n",
      "Epoch -  82 Loss -  0.05816324657344914\n",
      "Epoch -  83 Loss -  0.05735279705519912\n",
      "Epoch -  84 Loss -  0.05658172242193832\n",
      "Epoch -  85 Loss -  0.05584801690261077\n",
      "Epoch -  86 Loss -  0.055149779721652505\n",
      "Epoch -  87 Loss -  0.05448520952103197\n",
      "Epoch -  88 Loss -  0.05385259908093451\n",
      "Epoch -  89 Loss -  0.05325033032302827\n",
      "Epoch -  90 Loss -  0.05267686958113585\n",
      "Epoch -  91 Loss -  0.05213076312493031\n",
      "Epoch -  92 Loss -  0.05161063292306521\n",
      "Epoch -  93 Loss -  0.051115172632870756\n",
      "Epoch -  94 Loss -  0.05064314380444961\n",
      "Epoch -  95 Loss -  0.05019337228764908\n",
      "Epoch -  96 Loss -  0.04976474483102114\n",
      "Epoch -  97 Loss -  0.04935620586245409\n",
      "Epoch -  98 Loss -  0.04896675444172734\n",
      "Epoch -  99 Loss -  0.04859544137575677\n",
      "Epoch -  100 Loss -  0.04824136648779756\n",
      "Epoch -  101 Loss -  0.04790367603234491\n",
      "Epoch -  102 Loss -  0.04758156024791327\n",
      "Epoch -  103 Loss -  0.04727425104029624\n",
      "Epoch -  104 Loss -  0.04698101978930947\n",
      "Epoch -  105 Loss -  0.04670117527239377\n",
      "Epoch -  106 Loss -  0.046434061698811924\n",
      "Epoch -  107 Loss -  0.04617905684851045\n",
      "Epoch -  108 Loss -  0.045935570310037505\n",
      "Epoch -  109 Loss -  0.04570304181220378\n",
      "Epoch -  110 Loss -  0.04548093964447193\n",
      "Epoch -  111 Loss -  0.04526875916131193\n",
      "Epoch -  112 Loss -  0.04506602136603412\n",
      "Epoch -  113 Loss -  0.04487227156983703\n",
      "Epoch -  114 Loss -  0.04468707812204891\n",
      "Epoch -  115 Loss -  0.044510031207750406\n",
      "Epoch -  116 Loss -  0.0443407417091718\n",
      "Epoch -  117 Loss -  0.04417884012745498\n",
      "Epoch -  118 Loss -  0.04402397556155209\n",
      "Epoch -  119 Loss -  0.04387581474120266\n",
      "Epoch -  120 Loss -  0.043734041111103364\n",
      "Epoch -  121 Loss -  0.043598353963530366\n",
      "Epoch -  122 Loss -  0.043468467616826635\n",
      "Epoch -  123 Loss -  0.04334411063730938\n",
      "Epoch -  124 Loss -  0.04322502510226926\n",
      "Epoch -  125 Loss -  0.043110965901879225\n",
      "Epoch -  126 Loss -  0.04300170007792786\n",
      "Epoch -  127 Loss -  0.04289700619741843\n",
      "Epoch -  128 Loss -  0.04279667375917252\n",
      "Epoch -  129 Loss -  0.042700502631675775\n",
      "Epoch -  130 Loss -  0.04260830252050735\n",
      "Epoch -  131 Loss -  0.04251989246377228\n",
      "Epoch -  132 Loss -  0.042435100354044165\n",
      "Epoch -  133 Loss -  0.04235376248541393\n",
      "Epoch -  134 Loss -  0.04227572312430078\n",
      "Epoch -  135 Loss -  0.04220083410276619\n",
      "Epoch -  136 Loss -  0.0421289544331326\n",
      "Epoch -  137 Loss -  0.04205994994277759\n",
      "Epoch -  138 Loss -  0.04199369292802515\n",
      "Epoch -  139 Loss -  0.041930061826126186\n",
      "Epoch -  140 Loss -  0.04186894090436733\n",
      "Epoch -  141 Loss -  0.041810219965395044\n",
      "Epoch -  142 Loss -  0.04175379406790054\n",
      "Epoch -  143 Loss -  0.041699563261846456\n",
      "Epoch -  144 Loss -  0.041647432337470446\n",
      "Epoch -  145 Loss -  0.04159731058732985\n",
      "Epoch -  146 Loss -  0.04154911158070356\n",
      "Epoch -  147 Loss -  0.04150275294969102\n",
      "Epoch -  148 Loss -  0.041458156186397456\n",
      "Epoch -  149 Loss -  0.041415246450612134\n",
      "Epoch -  150 Loss -  0.04137395238743047\n",
      "Epoch -  151 Loss -  0.041334205954295426\n",
      "Epoch -  152 Loss -  0.04129594225695653\n",
      "Epoch -  153 Loss -  0.041259099393881886\n",
      "Epoch -  154 Loss -  0.04122361830867638\n",
      "Epoch -  155 Loss -  0.041189442650082266\n",
      "Epoch -  156 Loss -  0.04115651863916704\n",
      "Epoch -  157 Loss -  0.041124794943317666\n",
      "Epoch -  158 Loss -  0.04109422255668645\n",
      "Epoch -  159 Loss -  0.041064754686745454\n",
      "Epoch -  160 Loss -  0.041036346646636176\n",
      "Epoch -  161 Loss -  0.04100895575300289\n",
      "Epoch -  162 Loss -  0.040982541229028634\n",
      "Epoch -  163 Loss -  0.040957064112398675\n",
      "Epoch -  164 Loss -  0.04093248716793267\n",
      "Epoch -  165 Loss -  0.04090877480464685\n",
      "Epoch -  166 Loss -  0.0408858929970075\n",
      "Epoch -  167 Loss -  0.04086380921016552\n",
      "Epoch -  168 Loss -  0.04084249232895564\n",
      "Epoch -  169 Loss -  0.040821912590473644\n",
      "Epoch -  170 Loss -  0.04080204152003851\n",
      "Epoch -  171 Loss -  0.04078285187036583\n",
      "Epoch -  172 Loss -  0.0407643175637888\n",
      "Epoch -  173 Loss -  0.040746413637362054\n",
      "Epoch -  174 Loss -  0.04072911619070884\n",
      "Epoch -  175 Loss -  0.04071240233645712\n",
      "Epoch -  176 Loss -  0.040696250153144396\n",
      "Epoch -  177 Loss -  0.04068063864045339\n",
      "Epoch -  178 Loss -  0.040665547676662375\n",
      "Epoch -  179 Loss -  0.04065095797819793\n",
      "Epoch -  180 Loss -  0.04063685106117726\n",
      "Epoch -  181 Loss -  0.040623209204844565\n",
      "Epoch -  182 Loss -  0.040610015416795625\n",
      "Epoch -  183 Loss -  0.04059725339990825\n",
      "Epoch -  184 Loss -  0.0405849075208853\n",
      "Epoch -  185 Loss -  0.040572962780327404\n",
      "Epoch -  186 Loss -  0.0405614047842638\n",
      "Epoch -  187 Loss -  0.0405502197170583\n",
      "Epoch -  188 Loss -  0.04053939431562805\n",
      "Epoch -  189 Loss -  0.040528915844903715\n",
      "Epoch -  190 Loss -  0.040518772074470404\n",
      "Epoch -  191 Loss -  0.04050895125632614\n",
      "Epoch -  192 Loss -  0.04049944210370936\n",
      "Epoch -  193 Loss -  0.040490233770931\n",
      "Epoch -  194 Loss -  0.040481315834167864\n",
      "Epoch -  195 Loss -  0.04047267827316752\n",
      "Epoch -  196 Loss -  0.040464311453816786\n",
      "Epoch -  197 Loss -  0.040456206111533145\n",
      "Epoch -  198 Loss -  0.040448353335436366\n",
      "Epoch -  199 Loss -  0.04044074455326149\n",
      "Epoch -  200 Loss -  0.04043337151697678\n",
      "Epoch -  201 Loss -  0.04042622628907214\n",
      "Epoch -  202 Loss -  0.04041930122948349\n",
      "Epoch -  203 Loss -  0.04041258898312045\n",
      "Epoch -  204 Loss -  0.04040608246797363\n",
      "Epoch -  205 Loss -  0.04039977486376144\n",
      "Epoch -  206 Loss -  0.04039365960110444\n",
      "Epoch -  207 Loss -  0.04038773035118906\n",
      "Epoch -  208 Loss -  0.040381981015902534\n",
      "Epoch -  209 Loss -  0.040376405718415775\n",
      "Epoch -  210 Loss -  0.04037099879418999\n",
      "Epoch -  211 Loss -  0.04036575478238946\n",
      "Epoch -  212 Loss -  0.040360668417679525\n",
      "Epoch -  213 Loss -  0.04035573462238823\n",
      "Epoch -  214 Loss -  0.040350948499019906\n",
      "Epoch -  215 Loss -  0.04034630532309888\n",
      "Epoch -  216 Loss -  0.04034180053632942\n",
      "Epoch -  217 Loss -  0.04033742974005652\n",
      "Epoch -  218 Loss -  0.04033318868901363\n",
      "Epoch -  219 Loss -  0.040329073285342076\n",
      "Epoch -  220 Loss -  0.04032507957287132\n",
      "Epoch -  221 Loss -  0.040321203731647506\n",
      "Epoch -  222 Loss -  0.040317442072694276\n",
      "Epoch -  223 Loss -  0.04031379103300434\n",
      "Epoch -  224 Loss -  0.040310247170737966\n",
      "Epoch -  225 Loss -  0.040306807160633604\n",
      "Epoch -  226 Loss -  0.04030346778960592\n",
      "Epoch -  227 Loss -  0.04030022595253417\n",
      "Epoch -  228 Loss -  0.040297078648224904\n",
      "Epoch -  229 Loss -  0.04029402297554683\n",
      "Epoch -  230 Loss -  0.04029105612972135\n",
      "Epoch -  231 Loss -  0.04028817539877369\n",
      "Epoch -  232 Loss -  0.04028537816012388\n",
      "Epoch -  233 Loss -  0.040282661877321754\n",
      "Epoch -  234 Loss -  0.04028002409691642\n",
      "Epoch -  235 Loss -  0.04027746244544844\n",
      "Epoch -  236 Loss -  0.040274974626569274\n",
      "Epoch -  237 Loss -  0.04027255841827371\n",
      "Epoch -  238 Loss -  0.04027021167024415\n",
      "Epoch -  239 Loss -  0.040267932301302815\n",
      "Epoch -  240 Loss -  0.04026571829696212\n",
      "Epoch -  241 Loss -  0.04026356770707471\n",
      "Epoch -  242 Loss -  0.040261478643576105\n",
      "Epoch -  243 Loss -  0.04025944927831382\n",
      "Epoch -  244 Loss -  0.04025747784096445\n",
      "Epoch -  245 Loss -  0.040255562617027674\n",
      "Epoch -  246 Loss -  0.040253701945903025\n",
      "Epoch -  247 Loss -  0.04025189421903628\n",
      "Epoch -  248 Loss -  0.04025013787813819\n",
      "Epoch -  249 Loss -  0.04024843141347384\n",
      "Epoch -  250 Loss -  0.04024677336221312\n",
      "Epoch -  251 Loss -  0.04024516230684538\n",
      "Epoch -  252 Loss -  0.040243596873655176\n",
      "Epoch -  253 Loss -  0.04024207573125151\n",
      "Epoch -  254 Loss -  0.040240597589156005\n",
      "Epoch -  255 Loss -  0.04023916119644023\n",
      "Epoch -  256 Loss -  0.040237765340415246\n",
      "Epoch -  257 Loss -  0.040236408845369574\n",
      "Epoch -  258 Loss -  0.04023509057135002\n",
      "Epoch -  259 Loss -  0.04023380941299376\n",
      "Epoch -  260 Loss -  0.04023256429839367\n",
      "Epoch -  261 Loss -  0.040231354188013796\n",
      "Epoch -  262 Loss -  0.0402301780736368\n",
      "Epoch -  263 Loss -  0.040229034977353265\n",
      "Epoch -  264 Loss -  0.040227923950584804\n",
      "Epoch -  265 Loss -  0.040226844073142975\n",
      "Epoch -  266 Loss -  0.04022579445232103\n",
      "Epoch -  267 Loss -  0.04022477422201769\n",
      "Epoch -  268 Loss -  0.040223782541892614\n",
      "Epoch -  269 Loss -  0.04022281859654686\n",
      "Epoch -  270 Loss -  0.04022188159473956\n",
      "Epoch -  271 Loss -  0.04022097076862519\n",
      "Epoch -  272 Loss -  0.04022008537301862\n",
      "Epoch -  273 Loss -  0.04021922468468709\n",
      "Epoch -  274 Loss -  0.040218388001664884\n",
      "Epoch -  275 Loss -  0.040217574642591514\n",
      "Epoch -  276 Loss -  0.04021678394607349\n",
      "Epoch -  277 Loss -  0.040216015270065145\n",
      "Epoch -  278 Loss -  0.040215267991271765\n",
      "Epoch -  279 Loss -  0.04021454150457486\n",
      "Epoch -  280 Loss -  0.040213835222471205\n",
      "Epoch -  281 Loss -  0.04021314857453645\n",
      "Epoch -  282 Loss -  0.0402124810069018\n",
      "Epoch -  283 Loss -  0.04021183198175053\n",
      "Epoch -  284 Loss -  0.04021120097683092\n",
      "Epoch -  285 Loss -  0.040210587484984045\n",
      "Epoch -  286 Loss -  0.04020999101368726\n",
      "Epoch -  287 Loss -  0.04020941108461361\n",
      "Epoch -  288 Loss -  0.04020884723320252\n",
      "Epoch -  289 Loss -  0.04020829900825004\n",
      "Epoch -  290 Loss -  0.04020776597150497\n",
      "Epoch -  291 Loss -  0.040207247697283996\n",
      "Epoch -  292 Loss -  0.040206743772095696\n",
      "Epoch -  293 Loss -  0.04020625379427855\n",
      "Epoch -  294 Loss -  0.040205777373647666\n",
      "Epoch -  295 Loss -  0.04020531413115651\n",
      "Epoch -  296 Loss -  0.04020486369856602\n",
      "Epoch -  297 Loss -  0.04020442571812527\n",
      "Epoch -  298 Loss -  0.04020399984226336\n",
      "Epoch -  299 Loss -  0.04020358573328871\n",
      "Epoch -  300 Loss -  0.04020318306309879\n",
      "Epoch -  301 Loss -  0.04020279151290015\n",
      "Epoch -  302 Loss -  0.04020241077293438\n",
      "Epoch -  303 Loss -  0.04020204054221464\n",
      "Epoch -  304 Loss -  0.040201680528270174\n",
      "Epoch -  305 Loss -  0.04020133044689699\n",
      "Epoch -  306 Loss -  0.04020099002191884\n",
      "Epoch -  307 Loss -  0.040200658984952925\n",
      "Epoch -  308 Loss -  0.040200337075184486\n",
      "Epoch -  309 Loss -  0.04020002403914659\n",
      "Epoch -  310 Loss -  0.040199719630510056\n",
      "Epoch -  311 Loss -  0.040199423609873663\n",
      "Epoch -  312 Loss -  0.04019913574456868\n",
      "Epoch -  313 Loss -  0.04019885580846087\n",
      "Epoch -  314 Loss -  0.04019858358176523\n",
      "Epoch -  315 Loss -  0.0401983188508619\n",
      "Epoch -  316 Loss -  0.04019806140812139\n",
      "Epoch -  317 Loss -  0.04019781105173026\n",
      "Epoch -  318 Loss -  0.0401975675855265\n",
      "Epoch -  319 Loss -  0.04019733081883821\n",
      "Epoch -  320 Loss -  0.04019710056632422\n",
      "Epoch -  321 Loss -  0.04019687664782541\n",
      "Epoch -  322 Loss -  0.04019665888821492\n",
      "Epoch -  323 Loss -  0.04019644711725509\n",
      "Epoch -  324 Loss -  0.04019624116945872\n",
      "Epoch -  325 Loss -  0.04019604088395501\n",
      "Epoch -  326 Loss -  0.0401958461043567\n",
      "Epoch -  327 Loss -  0.04019565667863403\n",
      "Epoch -  328 Loss -  0.04019547245899159\n",
      "Epoch -  329 Loss -  0.040195293301747495\n",
      "Epoch -  330 Loss -  0.04019511906721802\n",
      "Epoch -  331 Loss -  0.04019494961960394\n",
      "Epoch -  332 Loss -  0.04019478482688077\n",
      "Epoch -  333 Loss -  0.040194624560693525\n",
      "Epoch -  334 Loss -  0.04019446869625127\n",
      "Epoch -  335 Loss -  0.04019431711222922\n",
      "Epoch -  336 Loss -  0.04019416969066843\n",
      "Epoch -  337 Loss -  0.040194026316883506\n",
      "Epoch -  338 Loss -  0.04019388687936936\n",
      "Epoch -  339 Loss -  0.04019375126971213\n",
      "Epoch -  340 Loss -  0.040193619382501905\n",
      "Epoch -  341 Loss -  0.04019349111525056\n",
      "Epoch -  342 Loss -  0.04019336636830713\n",
      "Epoch -  343 Loss -  0.040193245044779426\n",
      "Epoch -  344 Loss -  0.04019312705045843\n",
      "Epoch -  345 Loss -  0.040193012293739894\n",
      "Epoch -  346 Loss -  0.040192900685554596\n",
      "Epoch -  347 Loss -  0.04019279213929596\n",
      "Epoch -  348 Loss -  0.040192686570751895\n",
      "Epoch -  349 Loss -  0.04019258389803747\n",
      "Epoch -  350 Loss -  0.04019248404152928\n",
      "Epoch -  351 Loss -  0.0401923869238056\n",
      "Epoch -  352 Loss -  0.040192292469580226\n",
      "Epoch -  353 Loss -  0.04019220060564683\n",
      "Epoch -  354 Loss -  0.04019211126082013\n",
      "Epoch -  355 Loss -  0.04019202436587813\n",
      "Epoch -  356 Loss -  0.040191939853508676\n",
      "Epoch -  357 Loss -  0.04019185765825761\n",
      "Epoch -  358 Loss -  0.04019177771647455\n",
      "Epoch -  359 Loss -  0.04019169996626433\n",
      "Epoch -  360 Loss -  0.040191624347438434\n",
      "Epoch -  361 Loss -  0.0401915508014661\n",
      "Epoch -  362 Loss -  0.04019147927143054\n",
      "Epoch -  363 Loss -  0.04019140970198272\n",
      "Epoch -  364 Loss -  0.040191342039297695\n",
      "Epoch -  365 Loss -  0.04019127623103286\n",
      "Epoch -  366 Loss -  0.04019121222628724\n",
      "Epoch -  367 Loss -  0.040191149975562016\n",
      "Epoch -  368 Loss -  0.04019108943071876\n",
      "Epoch -  369 Loss -  0.04019103054494554\n",
      "Epoch -  370 Loss -  0.04019097327271809\n",
      "Epoch -  371 Loss -  0.04019091756976443\n",
      "Epoch -  372 Loss -  0.04019086339303086\n",
      "Epoch -  373 Loss -  0.04019081070064741\n",
      "Epoch -  374 Loss -  0.0401907594518964\n",
      "Epoch -  375 Loss -  0.0401907096071783\n",
      "Epoch -  376 Loss -  0.04019066112798365\n",
      "Epoch -  377 Loss -  0.04019061397686172\n",
      "Epoch -  378 Loss -  0.04019056811739011\n",
      "Epoch -  379 Loss -  0.04019052351414759\n",
      "Epoch -  380 Loss -  0.040190480132688\n",
      "Epoch -  381 Loss -  0.040190437939508744\n",
      "Epoch -  382 Loss -  0.04019039690203034\n",
      "Epoch -  383 Loss -  0.0401903569885669\n",
      "Epoch -  384 Loss -  0.040190318168303284\n",
      "Epoch -  385 Loss -  0.0401902804112708\n",
      "Epoch -  386 Loss -  0.04019024368832286\n",
      "Epoch -  387 Loss -  0.040190207971115006\n",
      "Epoch -  388 Loss -  0.04019017323207926\n",
      "Epoch -  389 Loss -  0.04019013944440565\n",
      "Epoch -  390 Loss -  0.04019010658201951\n",
      "Epoch -  391 Loss -  0.040190074619562324\n",
      "Epoch -  392 Loss -  0.040190043532371475\n",
      "Epoch -  393 Loss -  0.04019001329646114\n",
      "Epoch -  394 Loss -  0.04018998388850396\n",
      "Epoch -  395 Loss -  0.04018995528581231\n",
      "Epoch -  396 Loss -  0.04018992746632138\n",
      "Epoch -  397 Loss -  0.04018990040857187\n",
      "Epoch -  398 Loss -  0.04018987409169303\n",
      "Epoch -  399 Loss -  0.04018984849538609\n",
      "Epoch -  400 Loss -  0.04018982359991009\n",
      "Epoch -  401 Loss -  0.04018979938606494\n",
      "Epoch -  402 Loss -  0.04018977583517729\n",
      "Epoch -  403 Loss -  0.04018975292908614\n",
      "Epoch -  404 Loss -  0.04018973065012811\n",
      "Epoch -  405 Loss -  0.04018970898112451\n",
      "Epoch -  406 Loss -  0.04018968790536706\n",
      "Epoch -  407 Loss -  0.04018966740660669\n",
      "Epoch -  408 Loss -  0.04018964746903929\n",
      "Epoch -  409 Loss -  0.04018962807729368\n",
      "Epoch -  410 Loss -  0.040189609216420744\n",
      "Epoch -  411 Loss -  0.04018959087188019\n",
      "Epoch -  412 Loss -  0.04018957302953179\n",
      "Epoch -  413 Loss -  0.040189555675621366\n",
      "Epoch -  414 Loss -  0.04018953879677289\n",
      "Epoch -  415 Loss -  0.04018952237997668\n",
      "Epoch -  416 Loss -  0.04018950641257817\n",
      "Epoch -  417 Loss -  0.04018949088227272\n",
      "Epoch -  418 Loss -  0.0401894757770893\n",
      "Epoch -  419 Loss -  0.040189461085387714\n",
      "Epoch -  420 Loss -  0.040189446795845116\n",
      "Epoch -  421 Loss -  0.040189432897449376\n",
      "Epoch -  422 Loss -  0.040189419379491005\n",
      "Epoch -  423 Loss -  0.04018940623155225\n",
      "Epoch -  424 Loss -  0.040189393443501704\n",
      "Epoch -  425 Loss -  0.040189381005486066\n",
      "Epoch -  426 Loss -  0.04018936890792135\n",
      "Epoch -  427 Loss -  0.04018935714148573\n",
      "Epoch -  428 Loss -  0.040189345697113445\n",
      "Epoch -  429 Loss -  0.04018933456598667\n",
      "Epoch -  430 Loss -  0.04018932373952975\n",
      "Epoch -  431 Loss -  0.0401893132094009\n",
      "Epoch -  432 Loss -  0.040189302967487864\n",
      "Epoch -  433 Loss -  0.040189293005899886\n",
      "Epoch -  434 Loss -  0.040189283316962546\n",
      "Epoch -  435 Loss -  0.0401892738932116\n",
      "Epoch -  436 Loss -  0.04018926472738776\n",
      "Epoch -  437 Loss -  0.0401892558124298\n",
      "Epoch -  438 Loss -  0.04018924714147006\n",
      "Epoch -  439 Loss -  0.04018923870782919\n",
      "Epoch -  440 Loss -  0.04018923050501059\n",
      "Epoch -  441 Loss -  0.040189222526695746\n",
      "Epoch -  442 Loss -  0.040189214766738826\n",
      "Epoch -  443 Loss -  0.04018920721916276\n",
      "Epoch -  444 Loss -  0.04018919987815362\n",
      "Epoch -  445 Loss -  0.04018919273805751\n",
      "Epoch -  446 Loss -  0.040189185793374285\n",
      "Epoch -  447 Loss -  0.04018917903875496\n",
      "Epoch -  448 Loss -  0.04018917246899749\n",
      "Epoch -  449 Loss -  0.040189166079041515\n",
      "Epoch -  450 Loss -  0.040189159863965346\n",
      "Epoch -  451 Loss -  0.04018915381898182\n",
      "Epoch -  452 Loss -  0.04018914793943641\n",
      "Epoch -  453 Loss -  0.04018914222079961\n",
      "Epoch -  454 Loss -  0.04018913665866776\n",
      "Epoch -  455 Loss -  0.04018913124875735\n",
      "Epoch -  456 Loss -  0.04018912598690121\n",
      "Epoch -  457 Loss -  0.0401891208690473\n",
      "Epoch -  458 Loss -  0.04018911589125477\n",
      "Epoch -  459 Loss -  0.04018911104968956\n",
      "Epoch -  460 Loss -  0.040189106340622976\n",
      "Epoch -  461 Loss -  0.040189101760429176\n",
      "Epoch -  462 Loss -  0.04018909730558025\n",
      "Epoch -  463 Loss -  0.040189092972645855\n",
      "Epoch -  464 Loss -  0.04018908875828919\n",
      "Epoch -  465 Loss -  0.04018908465926515\n",
      "Epoch -  466 Loss -  0.04018908067241668\n",
      "Epoch -  467 Loss -  0.04018907679467399\n",
      "Epoch -  468 Loss -  0.04018907302305082\n",
      "Epoch -  469 Loss -  0.040189069354643156\n",
      "Epoch -  470 Loss -  0.040189065786625774\n",
      "Epoch -  471 Loss -  0.04018906231625119\n",
      "Epoch -  472 Loss -  0.04018905894084737\n",
      "Epoch -  473 Loss -  0.040189055657814526\n",
      "Epoch -  474 Loss -  0.04018905246462518\n",
      "Epoch -  475 Loss -  0.04018904935882035\n",
      "Epoch -  476 Loss -  0.04018904633800832\n",
      "Epoch -  477 Loss -  0.04018904339986322\n",
      "Epoch -  478 Loss -  0.04018904054212316\n",
      "Epoch -  479 Loss -  0.04018903776258617\n",
      "Epoch -  480 Loss -  0.040189035059113476\n",
      "Epoch -  481 Loss -  0.040189032429623024\n",
      "Epoch -  482 Loss -  0.04018902987208994\n",
      "Epoch -  483 Loss -  0.04018902738454494\n",
      "Epoch -  484 Loss -  0.04018902496507262\n",
      "Epoch -  485 Loss -  0.040189022611810336\n",
      "Epoch -  486 Loss -  0.04018902032294561\n",
      "Epoch -  487 Loss -  0.040189018096716815\n",
      "Epoch -  488 Loss -  0.04018901593140925\n",
      "Epoch -  489 Loss -  0.04018901382535583\n",
      "Epoch -  490 Loss -  0.04018901177693518\n",
      "Epoch -  491 Loss -  0.040189009784569574\n",
      "Epoch -  492 Loss -  0.040189007846725314\n",
      "Epoch -  493 Loss -  0.04018900596191055\n",
      "Epoch -  494 Loss -  0.04018900412867383\n",
      "Epoch -  495 Loss -  0.04018900234560351\n",
      "Epoch -  496 Loss -  0.04018900061132718\n",
      "Epoch -  497 Loss -  0.040188998924509084\n",
      "Epoch -  498 Loss -  0.04018899728385077\n",
      "Epoch -  499 Loss -  0.04018899568808876\n",
      "Epoch -  500 Loss -  0.04018899413599475\n",
      "Epoch -  501 Loss -  0.04018899262637354\n",
      "Epoch -  502 Loss -  0.040188991158062855\n",
      "Epoch -  503 Loss -  0.040188989729932165\n",
      "Epoch -  504 Loss -  0.04018898834088192\n",
      "Epoch -  505 Loss -  0.04018898698984262\n",
      "Epoch -  506 Loss -  0.04018898567577417\n",
      "Epoch -  507 Loss -  0.040188984397664905\n",
      "Epoch -  508 Loss -  0.040188983154530546\n",
      "Epoch -  509 Loss -  0.04018898194541441\n",
      "Epoch -  510 Loss -  0.04018898076938522\n",
      "Epoch -  511 Loss -  0.040188979625537274\n",
      "Epoch -  512 Loss -  0.040188978512990366\n",
      "Epoch -  513 Loss -  0.04018897743088808\n",
      "Epoch -  514 Loss -  0.040188976378396864\n",
      "Epoch -  515 Loss -  0.0401889753547069\n",
      "Epoch -  516 Loss -  0.040188974359029225\n",
      "Epoch -  517 Loss -  0.04018897339059808\n",
      "Epoch -  518 Loss -  0.04018897244866723\n",
      "Epoch -  519 Loss -  0.04018897153251225\n",
      "Epoch -  520 Loss -  0.040188970641427235\n",
      "Epoch -  521 Loss -  0.04018896977472594\n",
      "Epoch -  522 Loss -  0.0401889689317416\n",
      "Epoch -  523 Loss -  0.0401889681118254\n",
      "Epoch -  524 Loss -  0.04018896731434529\n",
      "Epoch -  525 Loss -  0.0401889665386874\n",
      "Epoch -  526 Loss -  0.040188965784255626\n",
      "Epoch -  527 Loss -  0.04018896505046819\n",
      "Epoch -  528 Loss -  0.040188964336760113\n",
      "Epoch -  529 Loss -  0.04018896364258234\n",
      "Epoch -  530 Loss -  0.04018896296739978\n",
      "Epoch -  531 Loss -  0.04018896231069371\n",
      "Epoch -  532 Loss -  0.04018896167195793\n",
      "Epoch -  533 Loss -  0.040188961050700286\n",
      "Epoch -  534 Loss -  0.04018896044644309\n",
      "Epoch -  535 Loss -  0.04018895985872092\n",
      "Epoch -  536 Loss -  0.040188959287081494\n",
      "Epoch -  537 Loss -  0.0401889587310844\n",
      "Epoch -  538 Loss -  0.040188958190301646\n",
      "Epoch -  539 Loss -  0.040188957664317154\n",
      "Epoch -  540 Loss -  0.04018895715272567\n",
      "Epoch -  541 Loss -  0.040188956655133556\n",
      "Epoch -  542 Loss -  0.04018895617115753\n",
      "Epoch -  543 Loss -  0.04018895570042538\n",
      "Epoch -  544 Loss -  0.040188955242573925\n",
      "Epoch -  545 Loss -  0.040188954797251805\n",
      "Epoch -  546 Loss -  0.04018895436411503\n",
      "Epoch -  547 Loss -  0.04018895394283087\n",
      "Epoch -  548 Loss -  0.04018895353307489\n",
      "Epoch -  549 Loss -  0.0401889531345313\n",
      "Epoch -  550 Loss -  0.040188952746893644\n",
      "Epoch -  551 Loss -  0.04018895236986321\n",
      "Epoch -  552 Loss -  0.04018895200315019\n",
      "Epoch -  553 Loss -  0.04018895164647171\n",
      "Epoch -  554 Loss -  0.040188951299553514\n",
      "Epoch -  555 Loss -  0.04018895096212824\n",
      "Epoch -  556 Loss -  0.04018895063393634\n",
      "Epoch -  557 Loss -  0.040188950314725316\n",
      "Epoch -  558 Loss -  0.04018895000424913\n",
      "Epoch -  559 Loss -  0.04018894970226865\n",
      "Epoch -  560 Loss -  0.040188949408551757\n",
      "Epoch -  561 Loss -  0.04018894912287214\n",
      "Epoch -  562 Loss -  0.04018894884500978\n",
      "Epoch -  563 Loss -  0.04018894857475071\n",
      "Epoch -  564 Loss -  0.040188948311887206\n",
      "Epoch -  565 Loss -  0.040188948056216645\n",
      "Epoch -  566 Loss -  0.04018894780754245\n",
      "Epoch -  567 Loss -  0.04018894756567303\n",
      "Epoch -  568 Loss -  0.04018894733042176\n",
      "Epoch -  569 Loss -  0.040188947101608086\n",
      "Epoch -  570 Loss -  0.04018894687905583\n",
      "Epoch -  571 Loss -  0.04018894666259321\n",
      "Epoch -  572 Loss -  0.04018894645205396\n",
      "Epoch -  573 Loss -  0.04018894624727588\n",
      "Epoch -  574 Loss -  0.04018894604810155\n",
      "Epoch -  575 Loss -  0.04018894585437743\n",
      "Epoch -  576 Loss -  0.040188945665954164\n",
      "Epoch -  577 Loss -  0.040188945482687144\n",
      "Epoch -  578 Loss -  0.04018894530443466\n",
      "Epoch -  579 Loss -  0.04018894513106019\n",
      "Epoch -  580 Loss -  0.04018894496242975\n",
      "Epoch -  581 Loss -  0.040188944798414034\n",
      "Epoch -  582 Loss -  0.04018894463888591\n",
      "Epoch -  583 Loss -  0.0401889444837233\n",
      "Epoch -  584 Loss -  0.040188944332806745\n",
      "Epoch -  585 Loss -  0.040188944186019836\n",
      "Epoch -  586 Loss -  0.040188944043249436\n",
      "Epoch -  587 Loss -  0.040188943904385904\n",
      "Epoch -  588 Loss -  0.040188943769322444\n",
      "Epoch -  589 Loss -  0.040188943637954826\n",
      "Epoch -  590 Loss -  0.04018894351018157\n",
      "Epoch -  591 Loss -  0.040188943385904695\n",
      "Epoch -  592 Loss -  0.040188943265028816\n",
      "Epoch -  593 Loss -  0.04018894314746055\n",
      "Epoch -  594 Loss -  0.040188943033109395\n",
      "Epoch -  595 Loss -  0.040188942921887044\n",
      "Epoch -  596 Loss -  0.04018894281370849\n",
      "Epoch -  597 Loss -  0.040188942708489994\n",
      "Epoch -  598 Loss -  0.04018894260615078\n",
      "Epoch -  599 Loss -  0.04018894250661185\n",
      "Epoch -  600 Loss -  0.04018894240979703\n",
      "Epoch -  601 Loss -  0.0401889423156313\n",
      "Epoch -  602 Loss -  0.04018894222404196\n",
      "Epoch -  603 Loss -  0.040188942134958924\n",
      "Epoch -  604 Loss -  0.04018894204831394\n",
      "Epoch -  605 Loss -  0.040188941964039514\n",
      "Epoch -  606 Loss -  0.04018894188207145\n",
      "Epoch -  607 Loss -  0.04018894180234631\n",
      "Epoch -  608 Loss -  0.04018894172480245\n",
      "Epoch -  609 Loss -  0.04018894164938096\n",
      "Epoch -  610 Loss -  0.04018894157602308\n",
      "Epoch -  611 Loss -  0.040188941504672435\n",
      "Epoch -  612 Loss -  0.04018894143527437\n",
      "Epoch -  613 Loss -  0.04018894136777516\n",
      "Epoch -  614 Loss -  0.04018894130212322\n",
      "Epoch -  615 Loss -  0.04018894123826781\n",
      "Epoch -  616 Loss -  0.040188941176159385\n",
      "Epoch -  617 Loss -  0.04018894111575063\n",
      "Epoch -  618 Loss -  0.04018894105699511\n",
      "Epoch -  619 Loss -  0.040188940999847426\n",
      "Epoch -  620 Loss -  0.04018894094426318\n",
      "Epoch -  621 Loss -  0.04018894089020017\n",
      "Epoch -  622 Loss -  0.04018894083761641\n",
      "Epoch -  623 Loss -  0.040188940786471256\n",
      "Epoch -  624 Loss -  0.04018894073672596\n",
      "Epoch -  625 Loss -  0.04018894068834207\n",
      "Epoch -  626 Loss -  0.040188940641281895\n",
      "Epoch -  627 Loss -  0.04018894059550951\n",
      "Epoch -  628 Loss -  0.0401889405509898\n",
      "Epoch -  629 Loss -  0.040188940507688134\n",
      "Epoch -  630 Loss -  0.04018894046557142\n",
      "Epoch -  631 Loss -  0.04018894042460702\n",
      "Epoch -  632 Loss -  0.040188940384763824\n",
      "Epoch -  633 Loss -  0.04018894034601088\n",
      "Epoch -  634 Loss -  0.04018894030831825\n",
      "Epoch -  635 Loss -  0.040188940271657016\n",
      "Epoch -  636 Loss -  0.04018894023599901\n",
      "Epoch -  637 Loss -  0.04018894020131683\n",
      "Epoch -  638 Loss -  0.040188940167583745\n",
      "Epoch -  639 Loss -  0.040188940134773414\n",
      "Epoch -  640 Loss -  0.04018894010286123\n",
      "Epoch -  641 Loss -  0.04018894007182201\n",
      "Epoch -  642 Loss -  0.040188940041632495\n",
      "Epoch -  643 Loss -  0.04018894001226889\n",
      "Epoch -  644 Loss -  0.04018893998370873\n",
      "Epoch -  645 Loss -  0.04018893995593024\n",
      "Epoch -  646 Loss -  0.04018893992891148\n",
      "Epoch -  647 Loss -  0.0401889399026325\n",
      "Epoch -  648 Loss -  0.04018893987707245\n",
      "Epoch -  649 Loss -  0.04018893985221177\n",
      "Epoch -  650 Loss -  0.04018893982803144\n",
      "Epoch -  651 Loss -  0.0401889398045128\n",
      "Epoch -  652 Loss -  0.04018893978163754\n",
      "Epoch -  653 Loss -  0.040188939759388594\n",
      "Epoch -  654 Loss -  0.04018893973774838\n",
      "Epoch -  655 Loss -  0.04018893971669989\n",
      "Epoch -  656 Loss -  0.04018893969622788\n",
      "Epoch -  657 Loss -  0.04018893967631572\n",
      "Epoch -  658 Loss -  0.04018893965694872\n",
      "Epoch -  659 Loss -  0.04018893963811152\n",
      "Epoch -  660 Loss -  0.040188939619789914\n",
      "Epoch -  661 Loss -  0.04018893960196936\n",
      "Epoch -  662 Loss -  0.0401889395846367\n",
      "Epoch -  663 Loss -  0.040188939567778185\n",
      "Epoch -  664 Loss -  0.04018893955138096\n",
      "Epoch -  665 Loss -  0.04018893953543258\n",
      "Epoch -  666 Loss -  0.0401889395199207\n",
      "Epoch -  667 Loss -  0.040188939504833265\n",
      "Epoch -  668 Loss -  0.040188939490158296\n",
      "Epoch -  669 Loss -  0.04018893947588518\n",
      "Epoch -  670 Loss -  0.040188939462002554\n",
      "Epoch -  671 Loss -  0.04018893944850018\n",
      "Epoch -  672 Loss -  0.04018893943536678\n",
      "Epoch -  673 Loss -  0.04018893942259291\n",
      "Epoch -  674 Loss -  0.04018893941016856\n",
      "Epoch -  675 Loss -  0.04018893939808404\n",
      "Epoch -  676 Loss -  0.04018893938633072\n",
      "Epoch -  677 Loss -  0.04018893937489867\n",
      "Epoch -  678 Loss -  0.04018893936377924\n",
      "Epoch -  679 Loss -  0.04018893935296447\n",
      "Epoch -  680 Loss -  0.040188939342445336\n",
      "Epoch -  681 Loss -  0.04018893933221424\n",
      "Epoch -  682 Loss -  0.040188939322262966\n",
      "Epoch -  683 Loss -  0.04018893931258438\n",
      "Epoch -  684 Loss -  0.040188939303170385\n",
      "Epoch -  685 Loss -  0.04018893929401389\n",
      "Epoch -  686 Loss -  0.04018893928510811\n",
      "Epoch -  687 Loss -  0.04018893927644569\n",
      "Epoch -  688 Loss -  0.04018893926802074\n",
      "Epoch -  689 Loss -  0.04018893925982585\n",
      "Epoch -  690 Loss -  0.040188939251855585\n",
      "Epoch -  691 Loss -  0.04018893924410346\n",
      "Epoch -  692 Loss -  0.040188939236563165\n",
      "Epoch -  693 Loss -  0.040188939229229365\n",
      "Epoch -  694 Loss -  0.04018893922209626\n",
      "Epoch -  695 Loss -  0.04018893921515826\n",
      "Epoch -  696 Loss -  0.0401889392084103\n",
      "Epoch -  697 Loss -  0.040188939201846866\n",
      "Epoch -  698 Loss -  0.04018893919546291\n",
      "Epoch -  699 Loss -  0.040188939189253876\n",
      "Epoch -  700 Loss -  0.04018893918321483\n",
      "Epoch -  701 Loss -  0.04018893917734033\n",
      "Epoch -  702 Loss -  0.04018893917162714\n",
      "Epoch -  703 Loss -  0.04018893916607055\n",
      "Epoch -  704 Loss -  0.040188939160665633\n",
      "Epoch -  705 Loss -  0.040188939155408665\n",
      "Epoch -  706 Loss -  0.04018893915029548\n",
      "Epoch -  707 Loss -  0.0401889391453223\n",
      "Epoch -  708 Loss -  0.04018893914048541\n",
      "Epoch -  709 Loss -  0.04018893913578059\n",
      "Epoch -  710 Loss -  0.040188939131204506\n",
      "Epoch -  711 Loss -  0.04018893912675387\n",
      "Epoch -  712 Loss -  0.04018893912242494\n",
      "Epoch -  713 Loss -  0.040188939118214105\n",
      "Epoch -  714 Loss -  0.04018893911411901\n",
      "Epoch -  715 Loss -  0.04018893911013577\n",
      "Epoch -  716 Loss -  0.04018893910626144\n",
      "Epoch -  717 Loss -  0.040188939102493104\n",
      "Epoch -  718 Loss -  0.04018893909882826\n",
      "Epoch -  719 Loss -  0.0401889390952632\n",
      "Epoch -  720 Loss -  0.040188939091795786\n",
      "Epoch -  721 Loss -  0.04018893908842337\n",
      "Epoch -  722 Loss -  0.040188939085143184\n",
      "Epoch -  723 Loss -  0.04018893908195321\n",
      "Epoch -  724 Loss -  0.04018893907884975\n",
      "Epoch -  725 Loss -  0.04018893907583185\n",
      "Epoch -  726 Loss -  0.04018893907289649\n",
      "Epoch -  727 Loss -  0.040188939070040945\n",
      "Epoch -  728 Loss -  0.04018893906726392\n",
      "Epoch -  729 Loss -  0.040188939064562675\n",
      "Epoch -  730 Loss -  0.040188939061935394\n",
      "Epoch -  731 Loss -  0.04018893905938032\n",
      "Epoch -  732 Loss -  0.040188939056894836\n",
      "Epoch -  733 Loss -  0.0401889390544774\n",
      "Epoch -  734 Loss -  0.04018893905212632\n",
      "Epoch -  735 Loss -  0.04018893904983935\n",
      "Epoch -  736 Loss -  0.040188939047614836\n",
      "Epoch -  737 Loss -  0.04018893904545132\n",
      "Epoch -  738 Loss -  0.04018893904334741\n",
      "Epoch -  739 Loss -  0.040188939041300456\n",
      "Epoch -  740 Loss -  0.04018893903930984\n",
      "Epoch -  741 Loss -  0.040188939037373625\n",
      "Epoch -  742 Loss -  0.04018893903549085\n",
      "Epoch -  743 Loss -  0.040188939033659006\n",
      "Epoch -  744 Loss -  0.04018893903187717\n",
      "Epoch -  745 Loss -  0.040188939030144505\n",
      "Epoch -  746 Loss -  0.040188939028459006\n",
      "Epoch -  747 Loss -  0.04018893902681985\n",
      "Epoch -  748 Loss -  0.040188939025225384\n",
      "Epoch -  749 Loss -  0.040188939023674486\n",
      "Epoch -  750 Loss -  0.0401889390221663\n",
      "Epoch -  751 Loss -  0.04018893902069911\n",
      "Epoch -  752 Loss -  0.0401889390192721\n",
      "Epoch -  753 Loss -  0.04018893901788437\n",
      "Epoch -  754 Loss -  0.040188939016534614\n",
      "Epoch -  755 Loss -  0.04018893901522151\n",
      "Epoch -  756 Loss -  0.04018893901394451\n",
      "Epoch -  757 Loss -  0.04018893901270262\n",
      "Epoch -  758 Loss -  0.040188939011494264\n",
      "Epoch -  759 Loss -  0.04018893901031927\n",
      "Epoch -  760 Loss -  0.040188939009176264\n",
      "Epoch -  761 Loss -  0.04018893900806492\n",
      "Epoch -  762 Loss -  0.04018893900698356\n",
      "Epoch -  763 Loss -  0.04018893900593182\n",
      "Epoch -  764 Loss -  0.04018893900490886\n",
      "Epoch -  765 Loss -  0.040188939003914195\n",
      "Epoch -  766 Loss -  0.04018893900294643\n",
      "Epoch -  767 Loss -  0.04018893900200548\n",
      "Epoch -  768 Loss -  0.040188939001089934\n",
      "Epoch -  769 Loss -  0.04018893900019944\n",
      "Epoch -  770 Loss -  0.04018893899933354\n",
      "Epoch -  771 Loss -  0.04018893899849159\n",
      "Epoch -  772 Loss -  0.04018893899767219\n",
      "Epoch -  773 Loss -  0.040188938996875194\n",
      "Epoch -  774 Loss -  0.04018893899610029\n",
      "Epoch -  775 Loss -  0.04018893899534653\n",
      "Epoch -  776 Loss -  0.040188938994613344\n",
      "Epoch -  777 Loss -  0.0401889389939\n",
      "Epoch -  778 Loss -  0.040188938993206774\n",
      "Epoch -  779 Loss -  0.04018893899253216\n",
      "Epoch -  780 Loss -  0.04018893899187573\n",
      "Epoch -  781 Loss -  0.04018893899123773\n",
      "Epoch -  782 Loss -  0.04018893899061697\n",
      "Epoch -  783 Loss -  0.0401889389900131\n",
      "Epoch -  784 Loss -  0.040188938989425896\n",
      "Epoch -  785 Loss -  0.040188938988854644\n",
      "Epoch -  786 Loss -  0.04018893898829902\n",
      "Epoch -  787 Loss -  0.04018893898775858\n",
      "Epoch -  788 Loss -  0.040188938987233205\n",
      "Epoch -  789 Loss -  0.040188938986722156\n",
      "Epoch -  790 Loss -  0.040188938986225\n",
      "Epoch -  791 Loss -  0.04018893898574142\n",
      "Epoch -  792 Loss -  0.040188938985270845\n",
      "Epoch -  793 Loss -  0.040188938984813634\n",
      "Epoch -  794 Loss -  0.04018893898436855\n",
      "Epoch -  795 Loss -  0.04018893898393577\n",
      "Epoch -  796 Loss -  0.040188938983514555\n",
      "Epoch -  797 Loss -  0.04018893898310517\n",
      "Epoch -  798 Loss -  0.040188938982706965\n",
      "Epoch -  799 Loss -  0.040188938982319844\n",
      "Epoch -  800 Loss -  0.04018893898194319\n",
      "Epoch -  801 Loss -  0.04018893898157683\n",
      "Epoch -  802 Loss -  0.040188938981220335\n",
      "Epoch -  803 Loss -  0.04018893898087372\n",
      "Epoch -  804 Loss -  0.040188938980536465\n",
      "Epoch -  805 Loss -  0.04018893898020855\n",
      "Epoch -  806 Loss -  0.040188938979889434\n",
      "Epoch -  807 Loss -  0.0401889389795793\n",
      "Epoch -  808 Loss -  0.040188938979277555\n",
      "Epoch -  809 Loss -  0.04018893897898406\n",
      "Epoch -  810 Loss -  0.04018893897869865\n",
      "Epoch -  811 Loss -  0.040188938978421296\n",
      "Epoch -  812 Loss -  0.04018893897815078\n",
      "Epoch -  813 Loss -  0.04018893897788853\n",
      "Epoch -  814 Loss -  0.04018893897763278\n",
      "Epoch -  815 Loss -  0.04018893897738465\n",
      "Epoch -  816 Loss -  0.040188938977142805\n",
      "Epoch -  817 Loss -  0.04018893897690772\n",
      "Epoch -  818 Loss -  0.04018893897667887\n",
      "Epoch -  819 Loss -  0.04018893897645663\n",
      "Epoch -  820 Loss -  0.04018893897624058\n",
      "Epoch -  821 Loss -  0.04018893897603007\n",
      "Epoch -  822 Loss -  0.04018893897582548\n",
      "Epoch -  823 Loss -  0.04018893897562645\n",
      "Epoch -  824 Loss -  0.040188938975432895\n",
      "Epoch -  825 Loss -  0.04018893897524446\n",
      "Epoch -  826 Loss -  0.04018893897506182\n",
      "Epoch -  827 Loss -  0.040188938974883376\n",
      "Epoch -  828 Loss -  0.040188938974709904\n",
      "Epoch -  829 Loss -  0.040188938974541816\n",
      "Epoch -  830 Loss -  0.0401889389743777\n",
      "Epoch -  831 Loss -  0.04018893897421844\n",
      "Epoch -  832 Loss -  0.040188938974063414\n",
      "Epoch -  833 Loss -  0.04018893897391261\n",
      "Epoch -  834 Loss -  0.04018893897376598\n",
      "Epoch -  835 Loss -  0.04018893897362311\n",
      "Epoch -  836 Loss -  0.040188938973484536\n",
      "Epoch -  837 Loss -  0.04018893897334946\n",
      "Epoch -  838 Loss -  0.040188938973218236\n",
      "Epoch -  839 Loss -  0.0401889389730905\n",
      "Epoch -  840 Loss -  0.040188938972966576\n",
      "Epoch -  841 Loss -  0.040188938972845534\n",
      "Epoch -  842 Loss -  0.040188938972728114\n",
      "Epoch -  843 Loss -  0.04018893897261408\n",
      "Epoch -  844 Loss -  0.04018893897250284\n",
      "Epoch -  845 Loss -  0.040188938972394804\n",
      "Epoch -  846 Loss -  0.040188938972289555\n",
      "Epoch -  847 Loss -  0.04018893897218737\n",
      "Epoch -  848 Loss -  0.040188938972087945\n",
      "Epoch -  849 Loss -  0.040188938971991016\n",
      "Epoch -  850 Loss -  0.0401889389718971\n",
      "Epoch -  851 Loss -  0.040188938971805505\n",
      "Epoch -  852 Loss -  0.04018893897171666\n",
      "Epoch -  853 Loss -  0.04018893897162996\n",
      "Epoch -  854 Loss -  0.04018893897154589\n",
      "Epoch -  855 Loss -  0.04018893897146373\n",
      "Epoch -  856 Loss -  0.040188938971384154\n",
      "Epoch -  857 Loss -  0.040188938971306716\n",
      "Epoch -  858 Loss -  0.04018893897123118\n",
      "Epoch -  859 Loss -  0.04018893897115815\n",
      "Epoch -  860 Loss -  0.04018893897108667\n",
      "Epoch -  861 Loss -  0.04018893897101729\n",
      "Epoch -  862 Loss -  0.04018893897094993\n",
      "Epoch -  863 Loss -  0.04018893897088445\n",
      "Epoch -  864 Loss -  0.04018893897082066\n",
      "Epoch -  865 Loss -  0.0401889389707585\n",
      "Epoch -  866 Loss -  0.04018893897069802\n",
      "Epoch -  867 Loss -  0.040188938970639396\n",
      "Epoch -  868 Loss -  0.04018893897058257\n",
      "Epoch -  869 Loss -  0.04018893897052666\n",
      "Epoch -  870 Loss -  0.040188938970472876\n",
      "Epoch -  871 Loss -  0.04018893897042021\n",
      "Epoch -  872 Loss -  0.040188938970369174\n",
      "Epoch -  873 Loss -  0.04018893897031944\n",
      "Epoch -  874 Loss -  0.04018893897027101\n",
      "Epoch -  875 Loss -  0.04018893897022414\n",
      "Epoch -  876 Loss -  0.04018893897017818\n",
      "Epoch -  877 Loss -  0.04018893897013376\n",
      "Epoch -  878 Loss -  0.040188938970090786\n",
      "Epoch -  879 Loss -  0.04018893897004869\n",
      "Epoch -  880 Loss -  0.04018893897000753\n",
      "Epoch -  881 Loss -  0.04018893896996786\n",
      "Epoch -  882 Loss -  0.04018893896992891\n",
      "Epoch -  883 Loss -  0.040188938969891265\n",
      "Epoch -  884 Loss -  0.04018893896985494\n",
      "Epoch -  885 Loss -  0.040188938969818976\n",
      "Epoch -  886 Loss -  0.040188938969784455\n",
      "Epoch -  887 Loss -  0.04018893896975067\n",
      "Epoch -  888 Loss -  0.040188938969718\n",
      "Epoch -  889 Loss -  0.04018893896968609\n",
      "Epoch -  890 Loss -  0.04018893896965509\n",
      "Epoch -  891 Loss -  0.04018893896962481\n",
      "Epoch -  892 Loss -  0.040188938969595446\n",
      "Epoch -  893 Loss -  0.040188938969566976\n",
      "Epoch -  894 Loss -  0.04018893896953926\n",
      "Epoch -  895 Loss -  0.0401889389695122\n",
      "Epoch -  896 Loss -  0.0401889389694859\n",
      "Epoch -  897 Loss -  0.040188938969460505\n",
      "Epoch -  898 Loss -  0.040188938969435296\n",
      "Epoch -  899 Loss -  0.04018893896941147\n",
      "Epoch -  900 Loss -  0.040188938969387904\n",
      "Epoch -  901 Loss -  0.04018893896936504\n",
      "Epoch -  902 Loss -  0.040188938969342725\n",
      "Epoch -  903 Loss -  0.04018893896932137\n",
      "Epoch -  904 Loss -  0.04018893896930016\n",
      "Epoch -  905 Loss -  0.040188938969279595\n",
      "Epoch -  906 Loss -  0.040188938969259694\n",
      "Epoch -  907 Loss -  0.04018893896924047\n",
      "Epoch -  908 Loss -  0.040188938969221585\n",
      "Epoch -  909 Loss -  0.04018893896920351\n",
      "Epoch -  910 Loss -  0.040188938969185475\n",
      "Epoch -  911 Loss -  0.0401889389691685\n",
      "Epoch -  912 Loss -  0.04018893896915133\n",
      "Epoch -  913 Loss -  0.04018893896913503\n",
      "Epoch -  914 Loss -  0.04018893896911898\n",
      "Epoch -  915 Loss -  0.0401889389691035\n",
      "Epoch -  916 Loss -  0.04018893896908846\n",
      "Epoch -  917 Loss -  0.04018893896907391\n",
      "Epoch -  918 Loss -  0.04018893896905959\n",
      "Epoch -  919 Loss -  0.04018893896904559\n",
      "Epoch -  920 Loss -  0.040188938969032036\n",
      "Epoch -  921 Loss -  0.04018893896901909\n",
      "Epoch -  922 Loss -  0.04018893896900644\n",
      "Epoch -  923 Loss -  0.040188938968993816\n",
      "Epoch -  924 Loss -  0.040188938968981895\n",
      "Epoch -  925 Loss -  0.04018893896896996\n",
      "Epoch -  926 Loss -  0.04018893896895863\n",
      "Epoch -  927 Loss -  0.04018893896894757\n",
      "Epoch -  928 Loss -  0.04018893896893663\n",
      "Epoch -  929 Loss -  0.04018893896892612\n",
      "Epoch -  930 Loss -  0.04018893896891615\n",
      "Epoch -  931 Loss -  0.040188938968906046\n",
      "Epoch -  932 Loss -  0.04018893896889633\n",
      "Epoch -  933 Loss -  0.04018893896888677\n",
      "Epoch -  934 Loss -  0.040188938968878075\n",
      "Epoch -  935 Loss -  0.04018893896886895\n",
      "Epoch -  936 Loss -  0.04018893896886025\n",
      "Epoch -  937 Loss -  0.04018893896885161\n",
      "Epoch -  938 Loss -  0.04018893896884386\n",
      "Epoch -  939 Loss -  0.04018893896883564\n",
      "Epoch -  940 Loss -  0.04018893896882794\n",
      "Epoch -  941 Loss -  0.04018893896882037\n",
      "Epoch -  942 Loss -  0.04018893896881341\n",
      "Epoch -  943 Loss -  0.04018893896880604\n",
      "Epoch -  944 Loss -  0.04018893896879897\n",
      "Epoch -  945 Loss -  0.04018893896879239\n",
      "Epoch -  946 Loss -  0.04018893896878577\n",
      "Epoch -  947 Loss -  0.04018893896877925\n",
      "Epoch -  948 Loss -  0.04018893896877314\n",
      "Epoch -  949 Loss -  0.04018893896876728\n",
      "Epoch -  950 Loss -  0.04018893896876127\n",
      "Epoch -  951 Loss -  0.04018893896875571\n",
      "Epoch -  952 Loss -  0.040188938968750115\n",
      "Epoch -  953 Loss -  0.040188938968744536\n",
      "Epoch -  954 Loss -  0.040188938968739284\n",
      "Epoch -  955 Loss -  0.04018893896873421\n",
      "Epoch -  956 Loss -  0.040188938968729326\n",
      "Epoch -  957 Loss -  0.040188938968724594\n",
      "Epoch -  958 Loss -  0.04018893896871976\n",
      "Epoch -  959 Loss -  0.04018893896871532\n",
      "Epoch -  960 Loss -  0.04018893896871083\n",
      "Epoch -  961 Loss -  0.04018893896870656\n",
      "Epoch -  962 Loss -  0.040188938968702195\n",
      "Epoch -  963 Loss -  0.04018893896869814\n",
      "Epoch -  964 Loss -  0.040188938968694084\n",
      "Epoch -  965 Loss -  0.040188938968690135\n",
      "Epoch -  966 Loss -  0.04018893896868645\n",
      "Epoch -  967 Loss -  0.040188938968682766\n",
      "Epoch -  968 Loss -  0.040188938968679255\n",
      "Epoch -  969 Loss -  0.04018893896867566\n",
      "Epoch -  970 Loss -  0.04018893896867252\n",
      "Epoch -  971 Loss -  0.040188938968669145\n",
      "Epoch -  972 Loss -  0.04018893896866606\n",
      "Epoch -  973 Loss -  0.04018893896866291\n",
      "Epoch -  974 Loss -  0.04018893896865975\n",
      "Epoch -  975 Loss -  0.040188938968657134\n",
      "Epoch -  976 Loss -  0.04018893896865403\n",
      "Epoch -  977 Loss -  0.04018893896865142\n",
      "Epoch -  978 Loss -  0.04018893896864856\n",
      "Epoch -  979 Loss -  0.040188938968646004\n",
      "Epoch -  980 Loss -  0.04018893896864341\n",
      "Epoch -  981 Loss -  0.04018893896864084\n",
      "Epoch -  982 Loss -  0.04018893896863855\n",
      "Epoch -  983 Loss -  0.0401889389686361\n",
      "Epoch -  984 Loss -  0.040188938968633875\n",
      "Epoch -  985 Loss -  0.04018893896863174\n",
      "Epoch -  986 Loss -  0.04018893896862962\n",
      "Epoch -  987 Loss -  0.04018893896862746\n",
      "Epoch -  988 Loss -  0.04018893896862521\n",
      "Epoch -  989 Loss -  0.040188938968623196\n",
      "Epoch -  990 Loss -  0.04018893896862144\n",
      "Epoch -  991 Loss -  0.04018893896861944\n",
      "Epoch -  992 Loss -  0.04018893896861779\n",
      "Epoch -  993 Loss -  0.040188938968616\n",
      "Epoch -  994 Loss -  0.040188938968614155\n",
      "Epoch -  995 Loss -  0.040188938968612434\n",
      "Epoch -  996 Loss -  0.040188938968610796\n",
      "Epoch -  997 Loss -  0.04018893896860931\n",
      "Epoch -  998 Loss -  0.040188938968607694\n",
      "Epoch -  999 Loss -  0.04018893896860636\n",
      "Epoch -  1000 Loss -  0.04018893896860449\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 0.13279574,  0.19464739],\n",
       "        [-0.03895788,  0.33389684]]),\n",
       " 'b1': array([[0.00396581],\n",
       "        [0.01746833]]),\n",
       " 'W2': array([[-0.16563886],\n",
       "        [ 0.61153427]]),\n",
       " 'b2': array([[0.6111508]])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# epochs implementation\n",
    "\n",
    "parameters = initialize_parameters([2,2,1])\n",
    "epochs = 50\n",
    "\n",
    "for i in range(epochs):\n",
    "\n",
    "    Loss = []\n",
    "\n",
    "    for j in range(df.shape[0]):\n",
    "\n",
    "        X = df[['cgpa', 'profile_score']].values[j].reshape(2,1) # Shape(no of features, no. of training example)\n",
    "        y = df[['lpa']].values[j][0]\n",
    "\n",
    "        # Parameter initialization\n",
    "\n",
    "\n",
    "        y_hat,A1 = L_layer_forward(X,parameters)\n",
    "        y_hat = y_hat[0][0]\n",
    "\n",
    "        update_parameters(parameters,y,y_hat,A1,X)\n",
    "\n",
    "        Loss.append((y-y_hat)**2)\n",
    "\n",
    "    print('Epoch - ',i+1,'Loss - ',np.array(Loss).mean())\n",
    "\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
